<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pages on Hugo tranquilpeak theme</title>
    <link>https://jialong952178959.github.io/tags/pages/</link>
    <description>Recent content in Pages on Hugo tranquilpeak theme</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 May 2018 22:04:26 +0800</lastBuildDate>
    
	<atom:link href="https://jialong952178959.github.io/tags/pages/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>https://jialong952178959.github.io/python%E7%88%AC%E8%99%AB/about/</link>
      <pubDate>Tue, 22 May 2018 22:04:26 +0800</pubDate>
      
      <guid>https://jialong952178959.github.io/python%E7%88%AC%E8%99%AB/about/</guid>
      <description> 大家好，我是渣渣辉。
简介  出生地：香港 性别：男 年龄：38  演员经历  是兄弟就来啃我 是兄弟就来啃我啊 是兄弟就来啃我啊哈哈  </description>
    </item>
    
    <item>
      <title>获取微信公众号文章踩坑记录（2）.md</title>
      <link>https://jialong952178959.github.io/python%E7%88%AC%E8%99%AB/selemium%E8%8E%B7%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%952/</link>
      <pubDate>Tue, 22 May 2018 22:04:26 +0800</pubDate>
      
      <guid>https://jialong952178959.github.io/python%E7%88%AC%E8%99%AB/selemium%E8%8E%B7%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%952/</guid>
      <description>在文章的开头贴上搜到的一个视频资源，关于爬取微信公众号文章。讲得比较细，用requests这个库。借鉴了里面的关于查看100条信息以后的登录的方法。虽然没搞懂为什么。
https://www.bilibili.com/video/av22267490/
  ## 分析文章列表
在文章列表中，每一篇文章都放在一个“&amp;lt;li&amp;gt;”标签下。 1. “文章时间”元素的id为“sogou_vr_11002601_box_0”到“sogou_vr_11002601_box_n”； 2. “文章链接”的id为“sogou_vr_11002601_title_x”，保存在href属性中； 3. “文章标题”也包含在“sogou_vr_11002601_title_x中”，因为含有搜索的关键字，整个标题被拆分成几大块放在不同位置；
因此先通过id找到标签元素，然后通过获取属性或者html代码的方式，获得三种要爬取的信息。循环执行，直到把当前页的信息全部获取。
## 循环次数的确定
循环的次数就是“&amp;lt;li&amp;gt;”标签的数量。使用find_elements_by_xpath()函数可返回文章列表的所有element得到一个List。元组的长度就是标签的数量。 ## 查看某个元素的xpath 在浏览器中右键&amp;ndash;》审查元素&amp;ndash;》在控制台中右键元素对应的标签代码&amp;ndash;》copy&amp;ndash;》copy xpath ## 注意时间标签的xpath路径 一般路径为&amp;rdquo;//*[@id=\&amp;ldquo;sogou_vr_11002601_box_0\&amp;ldquo;]/div[2]/div/span&amp;rdquo;，有的时候这个路径会搜不到，使用try except（异常捕捉），在搜不到的时候使用路径&amp;rdquo;//*[@id=\&amp;ldquo;sogou_vr_11002601_box_0\&amp;ldquo;]/div/div[2]/span&amp;rdquo;从新搜索。 ## 标题的拼接 获取html源码后删除里面的非标签内容。
标签的html代码： 删除里面的括号和括号里的内容即可
&amp;lt;em&amp;gt;&amp;lt;!--red_beg--&amp;gt;DataCastle&amp;lt;!--red_end--&amp;gt;&amp;lt;/em&amp;gt;应邀参加&amp;lt;em&amp;gt;&amp;lt;!--red_beg--&amp;gt;2016&amp;lt;!--red_end--&amp;gt;&amp;lt;/em&amp;gt;贵阳数博会,&amp;lt;em&amp;gt;&amp;lt;!--red_beg--&amp;gt;数据&amp;lt;!--red_end--&amp;gt;&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;&amp;lt;!--red_beg--&amp;gt;城堡&amp;lt;!--red_end--&amp;gt;&amp;lt;/em&amp;gt;堡主周涛现场展示大&amp;lt;em&amp;gt;&amp;lt;!--red_beg--&amp;gt;数据&amp;lt;!--red_end--&amp;gt;&amp;lt;/em&amp;gt;风采  replace（） 函数可以用空的字符串替代要删除的内容，达到删除的效果。
linkID=str(&#39;sogou_vr_11002601_title_&#39;+str(i)) link0=brower.find_element_by_id(linkID) #获取时间 strlink=link0.get_attribute(&#39;href&#39;) #获取标题的html代码 link0Html=str(link0.get_attribute(&#39;innerHTML&#39;)) replaceText=[&amp;quot;em&amp;quot;,&amp;quot;!--red_beg--&amp;quot;,&amp;quot;!--red_end--&amp;quot;,&amp;quot;&amp;lt;/&amp;gt;&amp;quot;,&amp;quot;&amp;lt;&amp;gt;&amp;quot;] #删除不需要的字符 for replaceTextEach in replaceText: link0Html=link0Html.replace(replaceTextEach,&amp;quot;&amp;quot;)  爬取的内容按时间排序   参考 https://www.cnblogs.com/whaben/p/6495702.html
   抓取的内容保存到List对象中，然后对List排序。sorted()函数可以根据字符排序，但是获得时间信息的格式如：“2018-6-10”在排序过程中与“2018-10-10”，会因为6&amp;gt;1，出现错误，因此把时间标准化为“20180610”。代码入下：
time0=brower.find_element_by_xpath(timeXpath) strTime0=str(time0.text) strTime0List=strTime0.split(&#39;-&#39;) #小于10在前面加0 if(int(strTime0List[1])&amp;lt;10): strTime0List[1]=&amp;quot;0&amp;quot;+strTime0List[1] if(int(strTime0List[2])&amp;lt;10): strTime0List[2]=&amp;quot;0&amp;quot;+strTime0List[2] strTime0=strTime0List[0]+strTime0List[1]+strTime0List[2]  关于find_element有时候selenium会报错“element is not attached to the page document” 网上的解释为，因为刷新或者网络延迟未加载出来，使得标签丢失。解决方法是在加载新页面或者刷新后让线程休息几秒。或者循环执行代码，直到找到找到标签。</description>
    </item>
    
    <item>
      <title>selemium获取微信公众号文章踩坑记录</title>
      <link>https://jialong952178959.github.io/python%E7%88%AC%E8%99%AB/selemium%E8%8E%B7%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Thu, 14 Dec 2017 11:18:15 +0800</pubDate>
      
      <guid>https://jialong952178959.github.io/python%E7%88%AC%E8%99%AB/selemium%E8%8E%B7%E5%8F%96%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</guid>
      <description>方法研究 搜狗搜索引擎可检索微信文章，通过公众号绑定的微信号搜索对应的文章。但是包含了其他公众号发出的包含该关键字的文章，好在搜狗自带了站内筛选功能，但是筛选后生产的URL不能直接作为，爬虫的静态页地址，貌似这属于一种反爬虫机制，原理相当于在未登录时直接通过链接进入邮箱，会被弹出到登录页面。因此需要模拟用户操作，模拟手动进行筛选。查到很多blog采用selenium这个python库，于是决定踩坑。
安装selenium  使用PIP命令 相关命令如下
win+r 输入cmd 然后执行
附上参考资料两篇：  https://www.cnblogs.com/sesshoumaru/p/python-selenium-webdriver.html  https://zhuanlan.zhihu.com/p/27115580 &amp;gt; 更新pip： python -m pip install &amp;ndash;upgrade pip
&amp;gt; 使cmd支持pip语法： 环境变量Path中加入python的Scripts路径（如：E:\python\Scripts）
&amp;gt; 安装seleniu： pip install selenium
  安装后在python的IDE里输入 “import selenium” 然后回车，如果没报错就是装好了。
使用浏览器引擎 浏览器引擎的作用是，自动控制浏览器，即模拟加载页面、点击、输入等人为的操作。因为电脑上装了Chrome，所以使用谷歌对应的驱动。下载地址：
http://chromedriver.storage.googleapis.com/index.html
使用的版本：2.38适配与Chrome 66
ps：这个引擎需要跟浏览器的版本匹配，不然会报错“call function result missing &amp;lsquo;value&amp;rsquo;”，调试时“终端”窗口打印。
下载后是exe文件，供python调用，所以不用安装。
python相关代码  引擎的调用
网上说放在系统环境变量里，但我试了，调试时报错找不到路径。解决办法：
就把chromedriver.exe放在python脚本的文件夹内，初始化方法如下：
 #coding=utf-8 from selenium import webdriver from selenium.webdriver.common.keys import Keys #通过传入路径初始化 ChromePath=&amp;quot;C:/Users/ZJL/Desktop/python/chromedriver.exe&amp;quot; brower=webdriver.Chrome(ChromePath) 
 加载微信搜索页面及自动筛选，这里以微信号为”DataCastle2016“为例： ``` #通过搜狗.</description>
    </item>
    
  </channel>
</rss>